# NLP Sentiment Analysis with IMDb Reviews

## ðŸ“‘ Overview
This project explores sentiment analysis using IMDb reviews dataset.
We compare:
- **Logistic Regression + TF-IDF** (baseline)
- **DistilBERT fine-tuning** (transfer learning)

## ðŸ“‚ Project Structure
nlp-sentiment-analysis/
â”œâ”€â”€ .venv/ # Virtual environment (not tracked in Git)
â”œâ”€â”€ .vscode/
â”‚ â””â”€â”€ settings.json # VS Code workspace settings
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ processed/
â”‚ â”‚ â”œâ”€â”€ test.csv
â”‚ â”‚ â”œâ”€â”€ train.csv
â”‚ â”‚ â””â”€â”€ valid.csv
â”‚ â””â”€â”€ raw/ # Raw data (empty or ignored)
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ results/ # Notebook output/results
â”‚ â”œâ”€â”€ 01_data_eda.ipynb # Exploratory data analysis
â”‚ â”œâ”€â”€ 02_baseline_model.ipynb # Logistic Regression baseline
â”‚ â”œâ”€â”€ 03_transformer_model.ipynb # DistilBERT fine-tuning
â”‚ â””â”€â”€ 04_evaluation_and_reporting.ipynb # Comparison & reporting
â”œâ”€â”€ reports/
â”‚ â””â”€â”€ figures/
â”‚ â””â”€â”€ model_comparison.png # Visualisation of model comparison
â”œâ”€â”€ src/ # Source code (future: inference scripts, utils)
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore # Ignore large/unnecessary files
â”œâ”€â”€ README.md # Project overview
â””â”€â”€ requirements.txt # Project dependencies
## ðŸ“Š Notebooks

**01_data_eda.ipynb**
- Load IMDB dataset
- Explore class balance, review lengths, sample text

**02_baseline_model.ipynb**
- Feature extraction with TFâ€“IDF
- Logistic Regression classifier
- Accuracy ~89%

**03_transformer_model.ipynb**
- Tokenisation with DistilBERT tokenizer
- Fine-tuned DistilBERT for sentiment classification
- Accuracy ~87% (subset, 2 epochs)

**04_evaluation_and_reporting.ipynb**
- Aggregate results
- Compare Logistic Regression vs DistilBERT (Accuracy, Precision, Recall, F1)
- Visualisation + analysis
- Discussion on trade-offs and future improvements

## ðŸ“ˆ Results  

| Model                   | Accuracy | Precision | Recall | F1   |
|-------------------------|----------|-----------|--------|------|
| Logistic Regression     | 0.892    | 0.89      | 0.89   | 0.89 |
| DistilBERT (subset, 2e) | 0.870    | ~0.86     | ~0.87  | ~0.87 |

![Model Comparison](reports/figures/model_comparison.png)

## ðŸš€ Future Work
- Train DistilBERT on full dataset
- Try BERT-base / RoBERTa
- Advanced evaluation (confusion matrices, ROC-AUC)
- Inference script for new reviews