{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eac6b7f",
   "metadata": {},
   "source": [
    "# NLP Sentiment Analysis - Step 3: Transformer Model: Fine tuning DistilBERT\n",
    "\n",
    "In the previous notebooks, we explored the IMDb dataset (`01_data_eda.ipynb`) and built a baseline Logistic Regression model using TF-IDF features (`02_baseline_model.ipynb`). While the baseline performed reasonablity well, it is limited in its ability to capture the deeper semantic meaning of text.\n",
    "In this notebook we will move beyond the baseline machine learning model and start fine-tuning a modern Transformer model (DistilBERT) for sentient classification.\n",
    "\n",
    "The goals of this step are:\n",
    "- Load a pre-trained DistilBERT model from HuggingFace\n",
    "- Tokenise the IMDb dataset using the model's tokeniser\n",
    "- Fine-tune DistilBERT on the training dataset\n",
    "- Evaluate the model on the validation and test sets\n",
    "- Compare results with the baseline Logistic Regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0d230",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "Import required libraries and load the IMDb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b08d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# Load the IMDb dataset (train, test, unsupervised)\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7babd6c0",
   "metadata": {},
   "source": [
    "## 2. Tokenisation\n",
    "Transformers cannot work directly with raw text.\n",
    "<br>Instead, they require token IDs that map to subword units.\n",
    "\n",
    "- load the pre-trained tokeniser for distilbert-base-uncased\n",
    "- apply tokenisation across the dataset\n",
    "- ensure each sequence has a fixed maximum length (e.g. 256 tokens)\n",
    "- use padding and truncation to handle reviews of different lengths\n",
    "\n",
    "This step transforms each review into the numeric format required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4c596c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f983d01a8af04c06b99987625b14dcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenise_function(examples):\n",
    "    return tokeniser(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "tokenised_datasets = dataset.map(tokenise_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640fa7ff",
   "metadata": {},
   "source": [
    "## 3. Model Definition and Training Setup\n",
    "\n",
    "Now the model and training configuration needs to be defined.\n",
    "\n",
    "-**Model**:\n",
    "DistilBERT is a smaller, fast varient of BERT that still retains roughly 97% of its language understanding capabilities. We add a classification head for **binary sentiment classification**.\n",
    "\n",
    "-**Training Arguments**:\n",
    "control the training process (batch size, number of epochs, evalution strategy, etc.).\n",
    "\n",
    "-**Metrics**:\n",
    "Use accuracy as our main evalution metric. The HuggingFace `evaluate` library makes this straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b388d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc218f",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n",
    "\n",
    "HuggingFace's high level trainer API is used which abstracts away most of the boilerplate code needed for training deep learning models.\n",
    "\n",
    "- Define the trainer obkect with our model, dataset, trainig arguments and evaluation metrics\n",
    "- Fine-tune DistilBERT on a subset of the training dataset (for quicker experimentation)\n",
    "- Monitor training and evaluation loss at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db5c90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\nlp-sentiment-analysis\\nlp-sentiment\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 40:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.368461</td>\n",
       "      <td>0.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.342227</td>\n",
       "      <td>0.859000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\nlp-sentiment-analysis\\nlp-sentiment\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.36060275268554687, metrics={'train_runtime': 2453.4945, 'train_samples_per_second': 1.63, 'train_steps_per_second': 0.102, 'total_flos': 264934797312000.0, 'train_loss': 0.36060275268554687, 'epoch': 2.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenised_datasets[\"train\"].shuffle(seed=42).select(range(2000)), # subset for speed\n",
    "    eval_dataset=tokenised_datasets[\"test\"].shuffle(seed=42).select(range(1000)),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a4fc7a",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation and Comparison\n",
    "\n",
    "### Logistic Regression (Baseline Model 2)\n",
    "- Test Accuracy: **89.2%**\n",
    "- Strengths: Lightweight, fast to train, surprisingly competitive performance on bag-of-words features\n",
    "- Weaknesses: Limited ability to capture semantic nuance (e.g. sarcasm, context beyond word frequency)\n",
    "\n",
    "### DistilBERT (Transformer Fine-tuning)\n",
    "- Validation Accuracy after 2 epochs: **86%**\n",
    "- Strengths: Captures context, word order, and nuanced semantics; generalises better on complex NLP tasks\n",
    "- Weaknesses: Training is slower and requires more compute; with our current setup, performance did not surpass logistic regression\n",
    "\n",
    "### Analysis\n",
    "Interestingly, the baseline **Logistic Regression slightly outperformed DistilBERT in this setup (89% vs 86%)**\n",
    "This highlights that:\n",
    "- Classical models can remain strong baselines, especially on well-structured datasets like IMDB\n",
    "- Pretrained transformers require careful fine-tuning (learning rate, batch size, number of epochs) to reach their full potential\n",
    "\n",
    "### Future Work\n",
    "- Train DistilBERT for longer (3â€“5 epochs) and adjust learning rate schedule\n",
    "- Try larger models like **BERT-base** or **RoBERTa** which are reported to exceed 90% accuracy on IMDB\n",
    "- Use regularisation (dropout, weight decay) to reduce overfitting\n",
    "- Explore data augmentation (e.g. back translation) to improve robustness\n",
    "\n",
    "Even though DistilBERT did not outperform the baseline in this experiment, the comparison provides valuable insights into the trade-offs between **classical ML** and **transformer-based models** in NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
